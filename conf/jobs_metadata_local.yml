examples/ex1_full_sql_job.sql:  # shows 100% sql job, easiest when sql is enough
  py_job: 'core/sql_job.py'
  inputs:
    some_events: {'path':"data/wiki_example/inputs/{latest}/events_log.csv.gz", 'type':'csv'}
    other_events: {'path':"data/wiki_example/inputs/{latest}/other_events_log.csv.gz", 'type':'csv'}
  output: {'path':'data/wiki_example/output_ex1_full_sql/{now}/', 'type':'csv'}

examples/ex1_frameworked_job.py:  # shows frameworked pyspark ops, same as ex1_full_sql_job but gives access to spark ops to expand on sql.
  inputs:
    some_events: {'path':"data/wiki_example/inputs/{latest}/events_log.csv.gz", 'type':'csv'}
    other_events: {'path':"data/wiki_example/inputs/{latest}/other_events_log.csv.gz", 'type':'csv'}
  output: {'path':'data/wiki_example/output_ex1_frameworked/{now}/', 'type':'csv'}

# ex1_raw_job:  # shows raw pyspark ops, no helper functions to deal with boilerplate. Job exists but doesn't rely on jobs_metadata entries

examples/ex2_frameworked_job.py:  # more complex version of ex1_frameworked_job
  inputs:
    some_events: {'path':"data/wiki_example/inputs/{latest}/events_log.csv.gz", 'type':'csv'}
    other_events: {'path':"data/wiki_example/inputs/{latest}/other_events_log.csv.gz", 'type':'csv'}
  output: {'path':'data/wiki_example/output_ex2/{now}/', 'type':'csv'}
  frequency: 1 day
  start_date: "{today}T07:00:00"
  email: 'some_email@address.com'

examples/ex3_incremental_job.py:  # focus on incremental loading and dropping
  inputs:
    processed_events: {'path':"data/wiki_example/output_ex3_prep/{latest}/", 'type':'csv', 'inc_field': 'timestamp_obj', 'from':'examples/ex3_incremental_prep_job.py'}  # TODO: check to not rely on dependency info here as it is provided lower.
  output: {'path':'data/wiki_example/output_ex3_inc/incremental_build_v2/', 'type':'csv', 'inc_field': 'other_timestamp'}
  frequency: 24h
  dependencies: [examples/ex3_incremental_prep_job.py]

examples/ex3_incremental_prep_job.py:  # shows computation of dependency as necessary for ex3_incremental_job
  inputs:
    some_events: {'path':"data/wiki_example/inputs/{latest}/events_log.csv.gz", 'type':'csv'}
  output: {'path':'data/wiki_example/output_ex3_prep/{now}/', 'type':'csv'}
  frequency: 24h

examples/ex4_dependency1_job.py:  # shows dependency
  inputs:
    some_events: {'path':"data/wiki_example/inputs/{latest}/events_log.csv.gz", 'type':'csv'}
  output: {'path':'data/wiki_example/output_ex4_dep1/{now}/', 'type':'csv'}

examples/ex4_dependency2_job.py:  # shows dependency
  inputs:
    some_events: {'from':'examples/ex4_dependency1_job.py'}
  output: {'path':'data/wiki_example/output_ex4_dep2/{now}/', 'type':'csv'}
  dependencies: [examples/ex4_dependency1_job.py]  # TODO: check to remove it since it is in input desc.

examples/ex4_dependency3_job.sql:  # shows dependency
  py_job: 'core/sql_job.py'
  inputs:
    some_events: {'path':'data/wiki_example/output_ex4_dep2/{latest}/', 'type':'csv', 'from':'examples/ex4_dependency2_job.py'}  # 'path' not needed when run as dependency
  output: {'path':'data/wiki_example/output_ex4_dep3/{now}/', 'type':'csv'}
  dependencies: [examples/ex4_dependency2_job.py, examples/ex4_dependency1_job.py]  # TODO: check to remove it since it is in input desc.

examples/ex4_dependency4_job.py:  # shows dependency
  inputs:
    some_events: {'path':'data/wiki_example/output_ex4_dep3/{latest}/', 'type':'csv', 'from':'examples/ex4_dependency3_job.sql'}
  output: {'path':'data/wiki_example/output_ex4_dep4/{now}/', 'type':'csv'}
  dependencies: [examples/ex4_dependency3_job.sql]  # TODO: check to remove it since it is in input desc.

examples/ex5_copy_to_oracle_job.py:  # shows frameworked pyspark ops, same as ex1_full_sql_job but gives access to spark ops to expand on sql.
  inputs:
    some_events: {'path':"data/wiki_example/inputs/{latest}/events_log.csv.gz", 'type':'csv'}
    other_events: {'path':"data/wiki_example/inputs/{latest}/other_events_log.csv.gz", 'type':'csv'}
  output: {'path':'data/wiki_example/output_ex5_copy_to_oracle/{now}/', 'type':'csv'}
  copy_to_oracle: {'creds': 'oracle', 'table': 'sandbox.test_ex5_pyspark_job'}

examples/ex5_copy_to_redshift_job.py:  # shows frameworked pyspark ops, same as ex1_full_sql_job but gives access to spark ops to expand on sql.
  inputs:
    some_events: {'path':"data/wiki_example/inputs/{latest}/events_log.csv.gz", 'type':'csv'}
    other_events: {'path':"data/wiki_example/inputs/{latest}/other_events_log.csv.gz", 'type':'csv'}
  output: {'path':'data/wiki_example/output_ex5_copy_to_oracle/{now}/', 'type':'csv'}
  copy_to_redshift: {'creds': 'some_redshift_cred_section', 'table': 'sandbox.test_ex5_pyspark_job'}

examples/ex6_mysql_job.py:  # requires mysql instance running
  api_inputs: {'api_creds': 'some_mysql_cred_section', 'note':'API Job that relies on creds from conf/connections.cfg'}
  output: {'path':'data/mysql_example/output_ex6_mysql/{now}/', 'type':'csv'}

examples/ex7_frameworked_job.py:  # shows job with no output (still requiring table as output but not dumped to disk)
  inputs:
    some_events: {'path':"data/wiki_example/inputs/{latest}/events_log.csv.gz", 'type':'csv'}
    other_events: {'path':"data/wiki_example/inputs/{latest}/other_events_log.csv.gz", 'type':'csv'}
  output: {'path':'n/a', 'type':'None'}

examples/ex8_koalas_job.py:
  inputs:
    some_events: {'path':"data/wiki_example/inputs/{latest}/events_log.csv.gz", 'type':'csv'}
    other_events: {'path':"data/wiki_example/inputs/{latest}/other_events_log.csv.gz", 'type':'csv'}
  output: {'path':'data/wiki_example/output_ex8_koalas/{now}/', 'type':'csv'}

examples/ex9_redshift_job.py:
  inputs:
    some_events: {'path':"data/wiki_example/inputs/{latest}/events_log.csv.gz", 'type':'csv'}
  output: {'path':'data/wiki_example/output_ex9_redshift/{now}/', 'type':'csv'}
  copy_to_redshift: {'creds': 'some_redshift_cred_section', 'table': 'sandbox.test_ex9_redshift'}

examples/ex9_mysql_job.py:
  db_inputs: {'creds': 'some_mysql_cred_section', 'note':'API Job that relies on creds from conf/connections.cfg'}
  output: {'path':'data/wiki_example/output_ex9_mysql/{now}/', 'type':'csv'}

examples/ex9_mysql_direct_job.py:
  inputs:
    some_events: {'type':'mysql', 'db_table': 'some_schema.some_table', 'creds': 'some_mysql_cred_section', 'note':'creds defined in conf/connections.cfg'}
  output: {'path':'data/wiki_example/output_ex9_mysql_direct/{now}/', 'type':'csv'}


examples/wordcount_frameworked_job.py:  # shows raw pyspark rdd ops in framework, same as wordcount_raw_job
  inputs:
    lines: {'path':"data/wordcount_example/input/sample_text.txt", 'type':'txt'}
  output: {'path':'data/wordcount_example/output_frameworked/{now}/', 'type':'txt'}

# wordcount_raw_job:  # shows raw pyspark rdd ops. Job exists but doesn't rely on jobs_metadata entries
