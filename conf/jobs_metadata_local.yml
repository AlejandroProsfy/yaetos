ex1_full_sql_job:  # shows 100% sql job, easiest when sql is enough
  inputs:
    some_events: {'path':"data/bogus_data/inputs/{latest}/events_log.csv.gz", 'type':'csv'}
    other_events: {'path':"data/bogus_data/inputs/{latest}/events_log.csv.gz", 'type':'csv'}
  output: {'path':'data/bogus_data/output_sql/{now}/', 'type':'csv'}

ex1_frameworked_job:  # shows frameworked pyspark ops, same as ex1_full_sql_job but gives access to spark ops to expand on sql.
  inputs:
    some_events: {'path':"data/bogus_data/inputs/{latest}/events_log.csv.gz", 'type':'csv'}
    other_events: {'path':"data/bogus_data/inputs/{latest}/events_log.csv.gz", 'type':'csv'}
  output: {'path':'data/bogus_data/output/{now}/', 'type':'csv'}

# ex1_raw_job:  # shows raw pyspark ops, no helper functions to deal with boilerplate. Job exists but doesn't rely on jobs_metadata entries

ex2_frameworked_job:  # more complex version of ex1_frameworked_job
  inputs:
    some_events: {'path':"data/bogus_data/inputs/{latest}/events_log.csv.gz", 'type':'csv'}
    other_events: {'path':"data/bogus_data/inputs/{latest}/events_log.csv.gz", 'type':'csv'}
  output: {'path':'data/bogus_data/output_ex2/{now}/', 'type':'csv'}

ex3_incremental_job:  # focus on incremental loading and dropping
  inputs:
    processed_events: {'path':"data/bogus_data/output_ex3_dep/{latest}/", 'type':'csv', 'inc_field': 'timestamp_obj'}
  output: {'path':'data/bogus_data/output_ex3_inc/incremental_build_v2/', 'type':'csv', 'inc_field': 'other_timestamp'}
  frequency: 24h

ex3_dependant_job:  # shows computation of dependency as necessary for ex3_incremental_job (manual for now)
  inputs:
    some_events: {'path':"data/bogus_data/inputs/{latest}/events_log_small.csv", 'type':'csv'}
  output: {'path':'data/bogus_data/output_ex3_dep/{now}/', 'type':'csv'}
  frequency: 24h

wordcount_frameworked_job:  # shows raw pyspark rdd ops in framework, same as wordcount_raw_job
  inputs:
    lines: {'path':"data/wordcount/inputs/sample_text.txt", 'type':'txt'}
  output: {'path':'data/wordcount/output/{now}/', 'type':'txt'}

# wordcount_raw_job:  # shows raw pyspark rdd ops. Job exists but doesn't rely on jobs_metadata entries
